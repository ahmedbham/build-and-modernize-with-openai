{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77448887-ed71-48e1-bf0d-2ff499d0c7ca",
   "metadata": {},
   "source": [
    "# Lesson 1 - Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ea165",
   "metadata": {},
   "source": [
    "Welcome to Lesson 1. \n",
    "\n",
    "To access the `requirement.txt` file, go to `File` and click on `Open`.\n",
    " \n",
    "I hope you enjoy this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750cd7dd",
   "metadata": {},
   "source": [
    "### Import the Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809aa032-d737-450d-aafa-e32bfba9d8f8",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42eb51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 9))\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting numpy==1.25.2 (from -r requirements.txt (line 11))\n",
      "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting pandas==2.1.3 (from -r requirements.txt (line 12))\n",
      "  Downloading pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scikit-learn==1.3.2 (from -r requirements.txt (line 13))\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting sentence-transformers==2.2.2 (from -r requirements.txt (line 14))\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib==3.8.2 (from -r requirements.txt (line 15))\n",
      "  Downloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting torch==2.1.1 (from -r requirements.txt (line 16))\n",
      "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting langchain==0.0.346 (from -r requirements.txt (line 18))\n",
      "  Downloading langchain-0.0.346-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting openai==0.28.1 (from -r requirements.txt (line 19))\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pinecone-client==3.0.0dev4 (from -r requirements.txt (line 21))\n",
      "  Downloading pinecone_client-3.0.0.dev4-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pinecone-datasets==0.5.0rc11 (from -r requirements.txt (line 22))\n",
      "  Downloading pinecone_datasets-0.5.0rc11-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pinecone-text==0.7.1 (from -r requirements.txt (line 23))\n",
      "  Downloading pinecone_text-0.7.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tiktoken==0.5.2 (from -r requirements.txt (line 25))\n",
      "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tqdm==4.66.1 (from -r requirements.txt (line 26))\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.15.0 (from -r requirements.txt (line 28))\n",
      "  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting deepface==0.0.79 (from -r requirements.txt (line 29))\n",
      "  Downloading deepface-0.0.79-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas==2.1.3->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas==2.1.3->-r requirements.txt (line 12)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas==2.1.3->-r requirements.txt (line 12)) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 13)) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from scikit-learn==1.3.2->-r requirements.txt (line 13)) (3.4.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers==2.2.2->-r requirements.txt (line 14))\n",
      "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision (from sentence-transformers==2.2.2->-r requirements.txt (line 14))\n",
      "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting nltk (from sentence-transformers==2.2.2->-r requirements.txt (line 14))\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sentencepiece (from sentence-transformers==2.2.2->-r requirements.txt (line 14))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from sentence-transformers==2.2.2->-r requirements.txt (line 14)) (0.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib==3.8.2->-r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib==3.8.2->-r requirements.txt (line 15)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib==3.8.2->-r requirements.txt (line 15)) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib==3.8.2->-r requirements.txt (line 15)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from matplotlib==3.8.2->-r requirements.txt (line 15)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib==3.8.2->-r requirements.txt (line 15)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from matplotlib==3.8.2->-r requirements.txt (line 15)) (3.1.2)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.1->-r requirements.txt (line 16))\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch==2.1.1->-r requirements.txt (line 16)) (12.1.105)\n",
      "Collecting triton==2.1.0 (from torch==2.1.1->-r requirements.txt (line 16))\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (3.9.5)\n",
      "Collecting anyio<4.0 (from langchain==0.0.346->-r requirements.txt (line 18))\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (0.6.6)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (1.33)\n",
      "Collecting langchain-core<0.1,>=0.0.10 (from langchain==0.0.346->-r requirements.txt (line 18))\n",
      "  Downloading langchain_core-0.0.13-py3-none-any.whl.metadata (978 bytes)\n",
      "Collecting langsmith<0.1.0,>=0.0.63 (from langchain==0.0.346->-r requirements.txt (line 18))\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain==0.0.346->-r requirements.txt (line 18)) (8.2.3)\n",
      "Requirement already satisfied: urllib3==2.0.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pinecone-client==3.0.0dev4->-r requirements.txt (line 21)) (2.0.7)\n",
      "Collecting fsspec (from torch==2.1.1->-r requirements.txt (line 16))\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting gcsfs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading gcsfs-2023.12.2.post1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting polars<0.17.0,>=0.16.4 (from pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading polars-0.16.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting pyarrow<12.0.0,>=11.0.0 (from pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.0.346->-r requirements.txt (line 18))\n",
      "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting s3fs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting mmh3<4.0.0,>=3.1.0 (from pinecone-text==0.7.1->-r requirements.txt (line 23))\n",
      "  Downloading mmh3-3.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Collecting wget<4.0,>=3.2 (from pinecone-text==0.7.1->-r requirements.txt (line 23))\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tiktoken==0.5.2->-r requirements.txt (line 25)) (2024.5.15)\n",
      "Collecting pyarrow-hotfix (from datasets==2.15.0->-r requirements.txt (line 28))\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0->-r requirements.txt (line 28))\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting xxhash (from datasets==2.15.0->-r requirements.txt (line 28))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.15.0->-r requirements.txt (line 28))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch==2.1.1->-r requirements.txt (line 16))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting gdown>=3.10.1 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting opencv-python>=4.5.5.64 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorflow>=1.9.0 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting keras>=2.2.0 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting Flask>=1.1.2 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting mtcnn>=0.1.0 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading mtcnn-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting retina-face>=0.0.1 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting fire>=0.4.0 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gunicorn>=20.1.0 (from deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1->-r requirements.txt (line 16)) (12.4.99)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.346->-r requirements.txt (line 18)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.346->-r requirements.txt (line 18)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.346->-r requirements.txt (line 18)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.346->-r requirements.txt (line 18)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.346->-r requirements.txt (line 18)) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.346->-r requirements.txt (line 18)) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.346->-r requirements.txt (line 18)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/codespace/.local/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.346->-r requirements.txt (line 18)) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.346->-r requirements.txt (line 18)) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.346->-r requirements.txt (line 18)) (0.9.0)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.10/site-packages (from fire>=0.4.0->deepface==0.0.79->-r requirements.txt (line 29)) (1.16.0)\n",
      "Collecting termcolor (from fire>=0.4.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask>=1.1.2->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask>=1.1.2->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from Flask>=1.1.2->deepface==0.0.79->-r requirements.txt (line 29)) (8.1.7)\n",
      "Collecting blinker>=1.6.2 (from Flask>=1.1.2->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: decorator>4.1.2 in /home/codespace/.local/lib/python3.10/site-packages (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (5.1.1)\n",
      "INFO: pip is looking at multiple versions of gcsfs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gcsfs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading gcsfs-2023.12.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading gcsfs-2023.12.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading gcsfs-2023.10.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (2.29.0)\n",
      "Collecting google-auth-oauthlib (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-storage (from gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.10/site-packages (from gdown>=3.10.1->deepface==0.0.79->-r requirements.txt (line 29)) (4.12.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch==2.1.1->-r requirements.txt (line 16)) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.346->-r requirements.txt (line 18)) (2.4)\n",
      "Collecting absl-py (from keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in /usr/local/python/3.10.13/lib/python3.10/site-packages (from keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29)) (13.7.1)\n",
      "Collecting namex (from keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting h5py (from keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting optree (from keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes (from keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.346->-r requirements.txt (line 18)) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.346->-r requirements.txt (line 18)) (2024.2.2)\n",
      "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading aiobotocore-2.13.0-py3-none-any.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs<2024.0.0,>=2023.1.0 (from pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.7.0 (from s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.346->-r requirements.txt (line 18)) (3.0.3)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29)) (24.3.25)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes (from keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29)) (68.2.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29)) (1.63.0)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 14)) (0.19.1)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->-r requirements.txt (line 14))\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.15.0->-r requirements.txt (line 28))\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch==2.1.1->-r requirements.txt (line 16)) (1.3.0)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from sentence-transformers==2.2.2->-r requirements.txt (line 14))\n",
      "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "  Downloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.31.65,>=1.31.16 (from aiobotocore~=2.7.0->s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading botocore-1.31.64-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore~=2.7.0->s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (4.9)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29)) (3.6)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow>=1.9.0->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.346->-r requirements.txt (line 18)) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.10/site-packages (from beautifulsoup4->gdown>=3.10.1->deepface==0.0.79->-r requirements.txt (line 29)) (2.5)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (2.0.0)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown>=3.10.1->deepface==0.0.79->-r requirements.txt (line 29))\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from rich->keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.10/site-packages (from rich->keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29)) (2.17.2)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (1.63.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22))\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface==0.0.79->-r requirements.txt (line 29)) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs<2024.0.0,>=2023.1.0->pinecone-datasets==0.5.0rc11->-r requirements.txt (line 22)) (3.2.2)\n",
      "Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.0.346-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_client-3.0.0.dev4-py3-none-any.whl (191 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.6/191.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_datasets-0.5.0rc11-py3-none-any.whl (12 kB)\n",
      "Downloading pinecone_text-0.7.1-py3-none-any.whl (21 kB)\n",
      "Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading deepface-0.0.79-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gcsfs-2023.10.0-py2.py3-none-any.whl (33 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.0.13-py3-none-any.whl (188 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.2/188.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-3.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38 kB)\n",
      "Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading polars-0.16.18-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
      "Downloading s3fs-2023.10.0-py3-none-any.whl (28 kB)\n",
      "Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiobotocore-2.7.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Downloading botocore-1.31.64-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence-transformers, fire, wget\n",
      "  Building wheel for sentence-transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=1156486d330b1632da22de272b418200ae9a014126487ad34e64684418f4c40d\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "  Building wheel for fire (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=ba96f4c66417575ac86e09a87d03493c81c40c4bc5fcafa4330ae8b106d67cd2\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
      "  Building wheel for wget (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=9395f26cb031fb06953c9c4e032ca0251fc8c3e732c794e3efbfe78342c8285a\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built sentence-transformers fire wget\n",
      "Installing collected packages: wget, sentencepiece, namex, mmh3, libclang, xxhash, wheel, Werkzeug, triton, tqdm, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, safetensors, python-dotenv, PySocks, pydantic, pyarrow-hotfix, proto-plus, polars, optree, nvidia-nccl-cu12, numpy, jmespath, itsdangerous, gunicorn, google-pasta, google-crc32c, gast, fsspec, dill, blinker, anyio, aioitertools, absl-py, tiktoken, tensorboard, pyarrow, pinecone-client, pandas, opt-einsum, opencv-python, nltk, multiprocess, ml-dtypes, langsmith, h5py, google-resumable-media, Flask, fire, botocore, astunparse, torch, scikit-learn, pinecone-text, openai, matplotlib, langchain-core, keras, google-auth-oauthlib, google-api-core, gdown, aiobotocore, transformers, torchvision, tensorflow, s3fs, mtcnn, langchain, google-cloud-core, datasets, sentence-transformers, retina-face, google-cloud-storage, gcsfs, deepface, pinecone-datasets\n",
      "  Attempting uninstall: mmh3\n",
      "    Found existing installation: mmh3 4.1.0\n",
      "    Uninstalling mmh3-4.1.0:\n",
      "      Successfully uninstalled mmh3-4.1.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.4\n",
      "    Uninstalling tqdm-4.66.4:\n",
      "      Successfully uninstalled tqdm-4.66.4\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 1.0.1\n",
      "    Uninstalling python-dotenv-1.0.1:\n",
      "      Successfully uninstalled python-dotenv-1.0.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.7.1\n",
      "    Uninstalling pydantic-2.7.1:\n",
      "      Successfully uninstalled pydantic-2.7.1\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.3.0\n",
      "    Uninstalling anyio-4.3.0:\n",
      "      Successfully uninstalled anyio-4.3.0\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.7.0\n",
      "    Uninstalling tiktoken-0.7.0:\n",
      "      Successfully uninstalled tiktoken-0.7.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.1\n",
      "    Uninstalling pandas-2.2.1:\n",
      "      Successfully uninstalled pandas-2.2.1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.59\n",
      "    Uninstalling langsmith-0.1.59:\n",
      "      Successfully uninstalled langsmith-0.1.59\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.2\n",
      "    Uninstalling torch-2.2.2:\n",
      "      Successfully uninstalled torch-2.2.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.4.1.post1\n",
      "    Uninstalling scikit-learn-1.4.1.post1:\n",
      "      Successfully uninstalled scikit-learn-1.4.1.post1\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.30.1\n",
      "    Uninstalling openai-1.30.1:\n",
      "      Successfully uninstalled openai-1.30.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.8.3\n",
      "    Uninstalling matplotlib-3.8.3:\n",
      "      Successfully uninstalled matplotlib-3.8.3\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.2.0\n",
      "    Uninstalling langchain-core-0.2.0:\n",
      "      Successfully uninstalled langchain-core-0.2.0\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.2.0\n",
      "    Uninstalling langchain-0.2.0:\n",
      "      Successfully uninstalled langchain-0.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.0 requires mmh3>=4.0.1, but you have mmh3 3.1.0 which is incompatible.\n",
      "langchain-community 0.2.0 requires langchain<0.3.0,>=0.2.0, but you have langchain 0.0.346 which is incompatible.\n",
      "langchain-community 0.2.0 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.0.13 which is incompatible.\n",
      "langchain-community 0.2.0 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.0.92 which is incompatible.\n",
      "langchain-text-splitters 0.2.0 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.0.13 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.0.3 PySocks-1.7.1 Werkzeug-3.0.3 absl-py-2.1.0 aiobotocore-2.7.0 aioitertools-0.11.0 anyio-3.7.1 astunparse-1.6.3 blinker-1.8.2 botocore-1.31.64 datasets-2.15.0 deepface-0.0.79 dill-0.3.7 fire-0.6.0 fsspec-2023.10.0 gast-0.5.4 gcsfs-2023.10.0 gdown-5.2.0 google-api-core-2.19.0 google-auth-oauthlib-1.2.0 google-cloud-core-2.4.1 google-cloud-storage-2.16.0 google-crc32c-1.5.0 google-pasta-0.2.0 google-resumable-media-2.7.0 gunicorn-22.0.0 h5py-3.11.0 itsdangerous-2.2.0 jmespath-1.0.1 keras-3.3.3 langchain-0.0.346 langchain-core-0.0.13 langsmith-0.0.92 libclang-18.1.1 matplotlib-3.8.2 ml-dtypes-0.3.2 mmh3-3.1.0 mtcnn-0.1.1 multiprocess-0.70.15 namex-0.0.8 nltk-3.8.1 numpy-1.25.2 nvidia-nccl-cu12-2.18.1 openai-0.28.1 opencv-python-4.9.0.80 opt-einsum-3.3.0 optree-0.11.0 pandas-2.1.3 pinecone-client-3.0.0.dev4 pinecone-datasets-0.5.0rc11 pinecone-text-0.7.1 polars-0.16.18 proto-plus-1.23.0 pyarrow-11.0.0 pyarrow-hotfix-0.6 pydantic-1.10.15 python-dotenv-1.0.0 retina-face-0.0.17 s3fs-2023.10.0 safetensors-0.4.3 scikit-learn-1.3.2 sentence-transformers-2.2.2 sentencepiece-0.2.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 tiktoken-0.5.2 torch-2.1.1 torchvision-0.16.1 tqdm-4.66.1 transformers-4.41.0 triton-2.1.0 wget-3.2 wheel-0.43.0 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30698fb9-4709-4088-9905-9ccb4efd5e09",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from DLAIUtils import Utils\n",
    "import DLAIUtils\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ab484bb-3bfb-4c52-a5bd-bcbe4a7a63d2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92fc2d",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce859e4b-9b50-4f53-b357-28d3e3872c87",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0d6a2eb27c4b64b2f7ef569cc58904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abac1bd7904e4b8a97333132866ab757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88226b88f23947e8b92394f4d73eff60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17fc60b9ca74c9ab08a10944ebec3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/58.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecb3299f54c4e3db06c75d7a1d49b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b019bbe831744bcb07f18272e8d7c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/404290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('quora', split='train[240000:290000]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356d4112-fa51-4092-9841-8b266e3b6a2c",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': [{'id': [207550, 351729],\n",
       "   'text': ['What is the truth of life?', \"What's the evil truth of life?\"]},\n",
       "  {'id': [33183, 351730],\n",
       "   'text': ['Which is the best smartphone under 20K in India?',\n",
       "    'Which is the best smartphone with in 20k in India?']},\n",
       "  {'id': [351731, 351732],\n",
       "   'text': ['Steps taken by Canadian government to improve literacy rate?',\n",
       "    'Can I send homemade herbal hair oil from India to US via postal or private courier services?']},\n",
       "  {'id': [37799, 94186],\n",
       "   'text': ['What is a good way to lose 30 pounds in 2 months?',\n",
       "    'What can I do to lose 30 pounds in 2 months?']},\n",
       "  {'id': [351733, 351734],\n",
       "   'text': ['Which of the following most accurately describes the translation of the graph y = (x+3)^2 -2 to the graph of y = (x -2)^2 +2?',\n",
       "    'How do you graph x + 2y = -2?']}],\n",
       " 'is_duplicate': [False, True, False, True, False]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332d1241-61ae-4d09-bf46-52081c133c0c",
   "metadata": {
    "height": 132
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the truth of life?\n",
      "What's the evil truth of life?\n",
      "Which is the best smartphone under 20K in India?\n",
      "Which is the best smartphone with in 20k in India?\n",
      "Steps taken by Canadian government to improve literacy rate?\n",
      "Can I send homemade herbal hair oil from India to US via postal or private courier services?\n",
      "What is a good way to lose 30 pounds in 2 months?\n",
      "What can I do to lose 30 pounds in 2 months?\n",
      "Which of the following most accurately describes the translation of the graph y = (x+3)^2 -2 to the graph of y = (x -2)^2 +2?\n",
      "How do you graph x + 2y = -2?\n",
      "--------------------------------------------------\n",
      "Number of questions: 100000\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "for record in dataset['questions']:\n",
    "    questions.extend(record['text'])\n",
    "question = list(set(questions))\n",
    "print('\\n'.join(questions[:10]))\n",
    "print('-' * 50)\n",
    "print(f'Number of questions: {len(questions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c0402",
   "metadata": {},
   "source": [
    "### Check cuda and Setup the model\n",
    "\n",
    "**Note**: \"Checking cuda\" refers to checking if you have access to GPUs (faster compute). In this course, we are using CPUs. So, you might notice some code cells taking a little longer to run.\n",
    "\n",
    "We are using *all-MiniLM-L6-v2* sentence-transformers model that maps sentences to a 384 dimensional dense vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb67759-ab38-4472-bfb0-4a56d1c05955",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry no cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87319c75a9b4572a71bb51b6f47cfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c7039763e64e0fbd274eea5ba0ce2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c629a8639994425a91748340283307d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dd79fc353e496faaf53614149f4bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984300e2227e4717bc4910ef00c541b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95e4f53820543ba81bba797825da61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18506aaf799459aa354bbdc5bcdff34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f430a72634247c8a7532c2a776f45f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7908bcf5b4bb42c9b3967539f87c1131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f6227c38a14e369b30f1c5a43e4def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f154c05fbb419bb08fa2a0121a90a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6900bba011af4ae49245b7c283a1f668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0622cc650f124c4bac2f56b036b17040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3bc40f16e14a69bd5925bfb75aee6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec286ee661024a44b415d0f796eb37e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device != 'cuda':\n",
    "    print('Sorry no cuda.')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90ec5ec-5397-4ed5-8163-7a901b6ecb0c",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'which city is the most populated in the world?'\n",
    "xq = model.encode(query)\n",
    "xq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780a189",
   "metadata": {},
   "source": [
    "### Setup Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3e3a94e-127f-4667-a9ae-7a17d7304ee6",
   "metadata": {
    "height": 43
   },
   "outputs": [],
   "source": [
    "utils = Utils()\n",
    "PINECONE_API_KEY = utils.get_pinecone_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a75eca-60f0-478b-bdcf-b68732c1545d",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl-ai-b7hg8vqjt3blbkfjawxrceekzfhy88wmdhoh\n",
      "<pinecone.data.index.Index object at 0x7b2442e2a080>\n"
     ]
    }
   ],
   "source": [
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "INDEX_NAME = utils.create_dlai_index_name('dl-ai')\n",
    "\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "print(INDEX_NAME)\n",
    "pinecone.create_index(name=INDEX_NAME, \n",
    "    dimension=model.get_sentence_embedding_dimension(), \n",
    "    metric='cosine',\n",
    "    spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
    "\n",
    "index = pinecone.Index(INDEX_NAME)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88d9424",
   "metadata": {},
   "source": [
    "### Create Embeddings and Upsert to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea551303-aa81-41cd-adc5-dc9ea8072397",
   "metadata": {
    "height": 352
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1341fcc14ae4e21b2b21745629dda5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size=200\n",
    "vector_limit=10000\n",
    "\n",
    "questions = question[:vector_limit]\n",
    "\n",
    "import json\n",
    "\n",
    "for i in tqdm(range(0, len(questions), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i+batch_size, len(questions))\n",
    "    # create IDs batch\n",
    "    ids = [str(x) for x in range(i, i_end)]\n",
    "    # create metadata batch\n",
    "    metadatas = [{'text': text} for text in questions[i:i_end]]\n",
    "    # create embeddings\n",
    "    xc = model.encode(questions[i:i_end])\n",
    "    # create records list for upsert\n",
    "    records = zip(ids, xc, metadatas)\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6153920a-f4c4-420e-9790-262dd2299fc6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 10000}},\n",
       " 'total_vector_count': 10000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71135418",
   "metadata": {},
   "source": [
    "### Run Your Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28b20b81-4782-4ce2-aec3-9576c7779f2e",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "# small helper function so we can repeat queries later\n",
    "def run_query(query):\n",
    "  embedding = model.encode(query).tolist()\n",
    "  results = index.query(top_k=10, vector=embedding, include_metadata=True, include_values=False)\n",
    "  for result in results['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c4244d6-7be0-4ee1-a36a-0d586f0555f7",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67: Which is the most urbanised city in India?\n",
      "0.64: What country has the fastest growing population and why?\n",
      "0.61: How's the world's population determined?\n",
      "0.56: What percentage of the world's population lives in developing countries?\n",
      "0.56: What is best city in India?\n",
      "0.56: Which is the best city in India?\n",
      "0.55: Where will the biggest increases in population come from the next 20 years?\n",
      "0.55: Where are the largest slums in the world?\n",
      "0.54: Which country is the world's largest democracy?\n",
      "0.53: What are the world`s deadliest tourist destinations?\n"
     ]
    }
   ],
   "source": [
    "run_query('which city has the highest population in the world?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f85b20fe-2328-47eb-84cd-bf3b27c6d4aa",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79: How do you add chocolate chips to cake mix?\n",
      "0.55: How do I bake a cake in a microwave oven?\n",
      "0.52: What should you do if your dog eats chocolate?\n",
      "0.49: How can I make a banana pudding without bananas?\n",
      "0.47: How do I make rice?\n",
      "0.44: Are chocolates good for health or not?\n",
      "0.43: What is the recipe for bruchetta?\n",
      "0.43: How do you cook sweet potatoes in a microwave?\n",
      "0.42: How do I cook sweet potato without oven or microwave?\n",
      "0.4: Where can I buy best quality ice cream cake & food display fridge in Sydney?\n"
     ]
    }
   ],
   "source": [
    "query = 'how do i make chocolate cake?'\n",
    "run_query(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
